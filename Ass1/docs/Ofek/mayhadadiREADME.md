# Distributed PDF Processing System

**Grade: 100/100**

## Team Members
- May Hadadi (hadadim@post.bgu.ac.il)
- Asaf Hacmon 

## Project Overview
This project implements a distributed system for processing PDF files in the AWS cloud. The system consists of three main components:
1. **Local Application** - Client-side application that submits jobs and receives results
2. **Manager** - Cloud-based coordinator that manages the workflow and workers
3. **Workers** - Cloud-based instances that perform the actual PDF processing

The system can scale dynamically based on workload and can process multiple client requests simultaneously.

## System Architecture

### Components
- **Local Application**: Initiates the process by uploading input files to S3, communicating with the Manager via SQS, and generating the final HTML output.
- **Manager**: The central coordinator that consists of 4 components:
  - **Manager Core**: Accepts new client requests and initializes necessary information
  - **Morker**: Thread pool that starts workers and distributes tasks
  - **Sort Manager**: Thread that processes completed tasks from workers
  - **Summary Manager**: Thread that creates summary files and sends them back to clients
- **Worker**: Processes PDF files using Apache PDFBox, supporting three operations:
  - ToImage - Convert the first page to PNG
  - ToHTML - Convert the first page to HTML
  - ToText - Convert the first page to text

### Communication
The system uses AWS SQS queues for message passing:
- LOCAL_TO_MANAGER: For communication from local app to manager
- MANAGER_TO_WORKER: For sending tasks from manager to workers
- WORKER_TO_MANAGER: For sending completed task results from workers to manager
- Client-specific queues: Created for sending results back to each client

### Data Storage
AWS S3 is used for storing:
- Input files from local applications
- PDF files processed by workers
- Output files (images, HTML, text) generated by workers
- Summary files prepared for clients

## How to Run

### Prerequisites
- Java 8 or higher
- Maven for building the project
- AWS CLI configured with appropriate credentials
- A predetermined S3 bucket (in this case: 315115261208393074b) for storing JAR files

### Setup
1. Build the project using Maven:
   ```
   mvn clean package
   ```

2. Upload the JAR files to the S3 bucket:
   ```
   aws s3 cp target/manager.jar s3://315115261208393074b/
   aws s3 cp target/worker.jar s3://315115261208393074b/
   ```

### Execution
Run the local application with the following command:
```
java -jar local-application.jar inputFileName outputFileName n [terminate]
```

Where:
- `inputFileName`: Path to the input file containing PDF URLs and operations
- `outputFileName`: Path where the output HTML file will be saved
- `n`: Number of PDF files per worker
- `terminate` (optional): If included, terminates the manager after completion

## Design Decisions and Implementation Details

### Scalability
- The system uses a thread pool for the manager to handle multiple client requests simultaneously
- Workers are dynamically created based on workload (n tasks per worker)
- The manager can scale up to process multiple input files in parallel

### Fault Tolerance
- If a worker fails, the message remains in the SQS queue and can be processed by another worker
- SQS message visibility timeout ensures tasks aren't lost if a worker crashes
- Workers handle exceptions and report errors back to the manager

### Security
- The system uses IAM roles for EC2 instances to access AWS services securely
- No AWS credentials are hardcoded in the application

### Performance Optimizations
- Thread pools are used for concurrent task processing
- Workers can process tasks independently without waiting for others
- Communication is asynchronous via SQS queues
- The system balances between latency and cost by creating just enough worker instances

## System Performance
- AMI used: ami-00e95a9222311e8ed
- Instance type: T2_NANO
- Processing time for input-sample-1: approximately 3 minutes
- Workers' files ratio used (n): [value]

## Limitations
- We used low-performance EC2 instances (T2_NANO) assuming that in real-life scenarios, we would be able to create more than 9 instances
- The maximum number of worker instances is capped at 7 to avoid AWS limits for student accounts
